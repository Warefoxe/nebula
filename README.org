* nebula

** requiremens
*** [[https://github.com/ggerganov/llama.cpp][llama.cpp]] backend
**** CPU
***** Windows
- [[https://github.com/skeeto/w64devkit/releases][w64devkit]]
-- Download the latest fortran version of [[https://github.com/skeeto/w64devkit/releases][w64devkit]].
-- Extract w64devkit.
-- Run ~w64devkit.exe~
**** Acceleration
***** CPU openBLAS
This provides BLAS acceleration using only the CPU.
****** Windows
******* [[https://github.com/skeeto/w64devkit/releases][w64devkit]]
- Download the latest fortran version of [[https://github.com/skeeto/w64devkit/releases][w64devkit]].
- Download the latest version of [[https://github.com/xianyi/OpenBLAS/releases][OpenBLAS for Windows]].
- Extract w64devkit.
- From the OpenBLAS zip that you just downloaded copy ~libopenblas.a~, located inside the ~lib~ folder, inside ~w64devkit\x86_64-w64-mingw32\lib~.
- From the same OpenBLAS zip copy the content of the ~include~ folder inside ~w64devkit\x86_64-w64-mingw32\include~.
- Run ~w64devkit.exe~.
***** NVIDIA GPU
This provides BLAS acceleration using the CUDA cores of your Nvidia GPU
****** [[https://developer.nvidia.com/cuda-downloads][Cuda Toolkit]]
***** AMD GPU
This provides BLAS acceleration on HIP-supported AMD GPUs.
****** [[https://rocm.docs.amd.com/en/latest/deploy/linux/quick_start.html][ROCm]]
** example [[https://github.com/nchapman/nebula/blob/main/examples/basic.rs][basic]]

*** code
#+BEGIN_SRC Rust
use nebula::{
    options::{ModelOptions, PredictOptions},
    Model,
};

fn main() {
    let model_options = ModelOptions::default().use_cpu();

    let mut model =
        Model::new("models/mistral-7b-instruct-v0.2.Q5_K_M.gguf", model_options).unwrap();

    let predict_options = PredictOptions::default().with_n_len(150);

    model
        .predict(
            "Write helloworld code in Rust.".into(),
            predict_options,
            Box::new(|token| {
                print!("{}", token);
                true
            }),
        )
        .unwrap();
    println!("");
}
#+END_SRC

*** model

#+BEGIN_SRC bash
  mkdir models
  cd models
  wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q5_K_M.gguf
  cd -
#+END_SRC

*** start with CPU
#+BEGIN_SRC bash
  cargo r --release --example basic
#+END_SRC
*** start with OpenBLAS
#+BEGIN_SRC bash
  cargo r --release --features openblas --example basic
#+END_SRC
*** start with CUDA
#+BEGIN_SRC bash
  cargo r --features cuda --release --example basic
#+END_SRC



** example [[https://github.com/nchapman/nebula/blob/main/examples/llava_1_6.rs][llava 1.6]]
*** code
#+BEGIN_SRC Rust
use nebula::{
    options::{ModelOptions, PredictOptions},
    Model,
};

fn main() {
    let model_options = ModelOptions::default().with_n_gpu_layers(10);

    let mut model = Model::new("models/llava-v1.6-mistral-7b.Q4_K_M.gguf", model_options).unwrap();

    let predict_options = PredictOptions::default().with_n_len(150);

    model
        .predict(
            "Write helloworld code in Rust.".into(),
            predict_options,
            Box::new(|token| {
                print!("{}", token);
                true
            }),
        )
        .unwrap();
    println!("");
}
#+END_SRC

*** model

#+BEGIN_SRC bash
  mkdir models
  cd models
  wget https://huggingface.co/cjpais/llava-1.6-mistral-7b-gguf/resolve/main/llava-v1.6-mistral-7b.Q4_K_M.gguf
  cd -
#+END_SRC

*** start with CPU
#+BEGIN_SRC bash
  cargo r --release --example llava_1_6
#+END_SRC
*** start with OpenBLAS
#+BEGIN_SRC bash
  cargo r --release --features openblas --example llava_1_6
#+END_SRC
*** start with CUDA
#+BEGIN_SRC bash
  cargo r --features cuda --release --example llava_1_6
#+END_SRC
